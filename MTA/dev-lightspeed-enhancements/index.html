<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Configuring and Using Developer Lightspeed for MTA</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.2"/><meta name="description" content="By using Developer Lightspeed for Migration Toolkit for Applications (MTA), you can modernize applications in your organization by applying LLM-driven code changes to resolve issues found through static code analysis. You can automate code fixes, review and apply the suggested code changes with minimum manual effort."/><link rel="next" href="#intro-to-the-developer-lightspeed_mta-developer-lightspeed" title="Chapter 1. Introduction to the Developer Lightspeed for MTA"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm45888100794224"><div class="titlepage"><div><div class="producttitle"><span class="productname">Migration Toolkit for Applications</span> <span class="productnumber">8.0</span></div><div><h1 class="title">Configuring and Using Developer Lightspeed for MTA</h1></div><div><h2 class="subtitle">Using the Migration Toolkit for Applications Developer Lightspeed to modernize your applications</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat Customer Content Services</span></div></div><div><a href="#idm45888077825088">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				By using Developer Lightspeed for Migration Toolkit for Applications (MTA), you can modernize applications in your organization by applying LLM-driven code changes to resolve issues found through static code analysis. You can automate code fixes, review and apply the suggested code changes with minimum manual effort.
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="chapter"><a href="#intro-to-the-developer-lightspeed_mta-developer-lightspeed">1. Introduction to the Developer Lightspeed for MTA</a></span><ul><li><span class="section"><a href="#use-case-ai-code-fix_mta-developer-lightspeed">1.1. Use case for AI-driven code fixes</a></span></li><li><span class="section"><a href="#how-developerlightspped-works_mta-developer-lightspeed">1.2. How does Developer Lightspeed for MTA work</a></span></li><li><span class="section"><a href="#modes-developer-lightspeed_mta-developer-lightspeed">1.3. Requesting code fixes in Developer Lightspeed for MTA</a></span></li><li><span class="section"><a href="#benefits-using-developer-lightspeed_mta-developer-lightspeed">1.4. Benefits of using Developer Lightspeed for MTA</a></span></li></ul></li><li><span class="chapter"><a href="#getting-started_mta-developer-lightspeed">2. Getting started with Developer Lightspeed for MTA</a></span><ul><li><span class="section"><a href="#prerequisites_getting-started">2.1. Prerequisites</a></span></li><li><span class="section"><a href="#persistent-volumes_getting-started">2.2. Persistent volume requirements</a></span></li><li><span class="section"><a href="#installation_getting-started">2.3. Installation</a></span></li><li><span class="section"><a href="#how-to-use-developer-lightspeed_getting-started">2.4. How to use Developer Lightspeed for MTA</a></span></li><li><span class="section"><a href="#example-code-generation_getting-started">2.5. Generating code fix suggestions example</a></span></li></ul></li><li><span class="chapter"><a href="#solution-server-configurations_mta-developer-lightspeed">3. Solution Server configurations</a></span><ul><li><span class="section"><a href="#tackle-llm-secret_solution-server-configurations">3.1. Configuring the model secret key</a></span></li><li><span class="section"><a href="#tackle-enable-dev-lightspeed_solution-server-configurations">3.2. Enabling Developer Lightspeed for MTA in Tackle custom resource</a></span></li></ul></li><li><span class="chapter"><a href="#configuring-llm_mta-developer-lightspeed">4. Configuring large language models for analysis</a></span><ul><li><span class="section"><a href="#llm-service-openshift-ai_configuring-llm">4.1. Deploying an LLM as a service in an OpenShift AI cluster</a></span></li><li><span class="section"><a href="#llm-provider-settings_configuring-llm">4.2. Configuring LLM provider settings</a></span></li><li><span class="section"><a href="#configuring-llm-podman_configuring-llm">4.3. Configuring the LLM in Podman Desktop</a></span></li></ul></li><li><span class="chapter"><a href="#configuring-dev-lightspeed-ide_mta-developer-lightspeed">5. Using MTA with Developer Lightspeed in IDE</a></span><ul><li><span class="section"><a href="#configuring-developer-lightspeed-ide-settings_configuring-dev-lightspeed-ide">5.1. Configuring the Developer Lightspeed for MTA IDE settings</a></span></li><li><span class="section"><a href="#configuring-solution-server-settings-file_configuring-dev-lightspeed-ide">5.2. Configuring the solution server settings</a></span></li><li><span class="section"><a href="#configuring-developer-lightspeed-profile-settings_configuring-dev-lightspeed-ide">5.3. Configuring the Developer Lightspeed for MTA profile settings</a></span></li></ul></li><li><span class="chapter"><a href="#run-dev-lightspeed-analysis_mta-developer-lightspeed">6. Running an analysis and resolving issues</a></span><ul><li><span class="section"><a href="#running-rag-analysis_run-dev-lightspeed-analysis">6.1. Running an Analysis</a></span></li><li><span class="section"><a href="#apply-rag-resolution_run-dev-lightspeed-analysis">6.2. Applying resolutions generated by the solution server</a></span></li><li><span class="section"><a href="#running-agent-analysis_run-dev-lightspeed-analysis">6.3. Generating code resolutions in the agent mode</a></span></li></ul></li><li><span class="chapter"><a href="#developer-lightspeed-logs_mta-developer-lightspeed">7. Debugging Developer Lightspeed for MTA</a></span><ul><li><span class="section"><a href="#dev-lightspeed-archive-logs_mta-developer-lightspeed">7.1. Archiving the logs</a></span></li><li><span class="section"><a href="#dev-lightspeed-access-logs_mta-developer-lightspeed">7.2. Accessing the logs</a></span></li></ul></li></ul></div><section class="chapter" id="intro-to-the-developer-lightspeed_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Introduction to the Developer Lightspeed for MTA</h1></div></div></div><p class="_abstract _abstract">
			Starting from 8.0.0, Migration Toolkit for Applications (MTA) integrates with large language models (LLM) through the Red Hat Developer Lightspeed for migration toolkit for applications component in the Visual Studio (VS) Code extension. You can use Developer Lightspeed for MTA to apply LLM-driven code changes to resolve issues found through static code analysis of Java applications.
		</p><section class="section" id="use-case-ai-code-fix_mta-developer-lightspeed"><div class="titlepage"><div><div><h2 class="title">1.1. Use case for AI-driven code fixes</h2></div></div></div><p>
				Migration Toolkit for Applications (MTA) performs the static code analysis for a specified target technology to which you want to migrate your applications. Red Hat provides 2400+ analysis rules in MTA for various Java technologies and you can extend the ruleset for custom frameworks or new technologies by creating custom rules.
			</p><p>
				The static code analysis describes the issues in your code that must be resolved. As you perform analysis for a large portfolio of applications, the issue description and the rule definition that may contain additional information form a large corpus of data that contains repetitive patterns of problem definitions and solutions.
			</p><p>
				Migrators do duplicate work by resolving issues that are repeated across applications in different migration waves.
			</p></section><section class="section" id="how-developerlightspped-works_mta-developer-lightspeed"><div class="titlepage"><div><div><h2 class="title">1.2. How does Developer Lightspeed for MTA work</h2></div></div></div><p>
				Developer Lightspeed for MTA works by collecting and storing the changes in the code for a large collection of applications, finding context to generate prompts for the LLM of your choice, and by generating code resolutions produced by the LLM to address specific issues.
			</p><p>
				Developer Lightspeed for MTA uses Retrieval Augmented Generation for context-based resolutions of issues in code. By using RAG, Developer Lightspeed for MTA improves the context shared with the LLM to generate more accurate suggestions to fix the issue in the code. The context allows the LLM to "reason" and generate suggestions for issues detected in the code. This mechanism helps to overcome the limited context size in LLMs that prevents them from analyzing the entire source code of an application.
			</p><p>
				The context is a combination of the source code, the issue description, and solved examples:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Description of issues detected by MTA when you run a static code analysis for a given set of target technologies.
					</li><li class="listitem">
						(Optional) The default and custom rules may contain additional information that you include which can help Developer Lightspeed for MTA to define the context.
					</li><li class="listitem"><p class="simpara">
						Solved examples constitute code changes from other migrations and a pattern of resolution for an issue that can be used in future. A solved example is created when a Migrator accepts a resolution in a previous analysis that results in updated code or an unfamiliar issue in a legacy application that the Migrator manually fixed. Solved examples are stored in the Solution Server.
					</p><p class="simpara">
						More instances of solved examples for an issue enhances the context and improve the success metrics of rules that trigger the issue. A higher success metrics of an issue refers to the higher confidence level associated with the accepted resolutions for that issue in previous analyses.
					</p></li><li class="listitem"><p class="simpara">
						(Optional) If you enable the Solution Server, it extracts a pattern of resolution, called the migration hint, that can be used by the LLM to generate a more accurate fix suggestion in a future analysis.
					</p><p class="simpara">
						The improvement in the quality of migration hints results in more accurate code resolutions. Accurate code resolutions from the LLM result in the user accepting an update to the code. The updated code is stored in the solution server to generate a better migration hint in future.
					</p><p class="simpara">
						This cyclical improvement of resolution pattern from the Solution Server and improved migration hints lead to more reliable code changes as you migrate applications in different migration waves.
					</p></li></ul></div></section><section class="section" id="modes-developer-lightspeed_mta-developer-lightspeed"><div class="titlepage"><div><div><h2 class="title">1.3. Requesting code fixes in Developer Lightspeed for MTA</h2></div></div></div><p>
				You can request AI-assisted code resolutions that obtain additional context from several potential sources, such as analysis issues, IDE diagnostic information, and past migration data via the Solution Server.
			</p><p>
				The Solution Server acts as an institutional memory that stores changes to source codes after analyzing applications in your organization. This helps you to leverage the recurring patterns of solutions for issues that are repeated in many applications.
			</p><p>
				When you use the Solution Server, Developer Lightspeed for MTA suggests a code resolution that is based on solved examples or code changes in past analysis. You can view a diff of the updated portions of the code and the original source code to do a manual review.
			</p><p>
				It also enables you to control the analysis through manual reviews of the suggested AI resolutions: you can accept, reject or edit the suggested code changes while reducing the overall time and effort required to prepare your application for migration.
			</p><p>
				In the agentic AI mode, Developer Lightspeed for MTA streams an automated analysis of the code in a loop until all issues are resolved and changes the code with the updates. In the initial run, the AI agent:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Plans the context to define the issues.
					</li><li class="listitem">
						Chooses a suitable sub agent for the analysis task. Works with the LLM to generate fix suggestions. The reasoning transcript and files to be changed are displayed to the user.
					</li><li class="listitem">
						Applies the changes to the code once the user approves the updates.
					</li></ul></div><p>
				If you accept that the agentic AI must continue to make changes, it compiles the code and runs a partial analysis. In this iteration, the agentic AI attempts to fix diagnostic issues (if any) generated by tools that you installed in the VS Code IDE. You can review the changes and accept the agentic AI’s suggestion to address these diagnostic issues.
			</p><p>
				After each iteration of applying changes to the code, the agentic AI asks if you want the agent to continue fixing more issue. When you accept, it runs another iteration of automated analysis until it has resolved all issues or it has made a maximum of two attempts to fix an issue.
			</p><p>
				Agentic AI generates a new preview in each iteration when it updates the code with the suggested resolutions. The time taken by the agentic AI to complete all iterations depends on the number of new diagnostic issues that are detected in the code.
			</p></section><section class="section" id="benefits-using-developer-lightspeed_mta-developer-lightspeed"><div class="titlepage"><div><div><h2 class="title">1.4. Benefits of using Developer Lightspeed for MTA</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Model agnostic</strong></span> - Developer Lightspeed for MTA follows a "Bring Your Own Model" approach, allowing your organization to use a preferred LLM.
					</li><li class="listitem">
						<span class="strong strong"><strong>Iterative refinement</strong></span> - Developer Lightspeed for MTA can include an agent that iterates through the source code to run a series of automated analyses that resolves both the code base and diagnostic issues.
					</li><li class="listitem">
						<span class="strong strong"><strong>Contextual code generation</strong></span> - By leveraging AI for static code analysis, Developer Lightspeed for MTA breaks down complex problems into more manageable ones, providing the LLM with focused context to generate meaningful results. This helps overcome the limited context size of LLMs when dealing with large codebases.
					</li><li class="listitem">
						<span class="strong strong"><strong>No fine tuning</strong></span> - You also do not need to fine tune your model with a suitable data set for analysis which leaves you free to use and switch LLM models to respond to your requirements.
					</li><li class="listitem">
						<span class="strong strong"><strong>Learning and Improvement</strong></span> - As more parts of a codebase are migrated with Developer Lightspeed for MTA, it can use RAG to learn from the available data and provide better recommendations in subsequent application analysis.
					</li></ul></div></section></section><section class="chapter" id="getting-started_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Getting started with Developer Lightspeed for MTA</h1></div></div></div><p class="_abstract _abstract">
			The Getting started section contains information to walk you through the prerequisites, persistent volume requirements, installation, and workflows that help you to decide how you want to use the Red Hat Developer Lightspeed for migration toolkit for applications.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				To get support for features in Developer Lightspeed for MTA, you require a Red Hat Advanced Developer Suite (RHADS) subscription.
			</p></div></div><section class="section" id="prerequisites_getting-started"><div class="titlepage"><div><div><h2 class="title">2.1. Prerequisites</h2></div></div></div><p class="_abstract _abstract">
				This section lists the prerequisites required to successfully use the generative AI features in the Developer Lightspeed for MTA Visual Studio (VS) Code extension.
			</p><p>
				Before you install Developer Lightspeed for MTA, you must:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Install Language Support for Java™ by Red Hat extension
					</li><li class="listitem">
						Install Java v17 and later
					</li><li class="listitem">
						Install Maven v3.9.9 or later
					</li><li class="listitem">
						Install Git and add it to the $PATH variable
					</li><li class="listitem"><p class="simpara">
						Install the MTA Operator 8.0.0
					</p><p class="simpara">
						The MTA Operator is mandatory if you plan to enable the solution server that works with the large language model (LLM) for generating code changes. It enables you to log in to the <code class="literal">openshift-mta</code> project where you must enable the Solution Server in the Tackle custom resources (CR).
					</p></li><li class="listitem"><p class="simpara">
						Create an API key for an LLM.
					</p><p class="simpara">
						You must enter the provider value and model name in Tackle CR to enable generative AI configuration in the MTA VS Code plugin.
					</p><div class="table" id="idm45888099123824"><p class="title"><strong>Table 2.1. Configurable large language models and providers</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45888098640352" scope="col">LLM Provider (Tackle CR value)</th><th align="left" valign="top" id="idm45888100748096" scope="col">Large language model examples for Tackle CR configuration</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45888098640352">
									<p>
										OpenShift AI platform
									</p>
									</td><td align="left" valign="top" headers="idm45888100748096">
									<p>
										Models deployed in an OpenShift AI cluster that can be accessed by using Open AI-compatible API.
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45888098640352">
									<p>
										Open AI (<code class="literal">openai</code>)
									</p>
									</td><td align="left" valign="top" headers="idm45888100748096">
									<p>
										<code class="literal">gpt-4</code>, <code class="literal">gpt-4o</code>, <code class="literal">gpt-4o-mini</code>, <code class="literal">gpt-3.5-turbo</code>
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45888098640352">
									<p>
										Azure OpenAI (<code class="literal">azure_openai</code>)
									</p>
									</td><td align="left" valign="top" headers="idm45888100748096">
									<p>
										<code class="literal">gpt-4</code>, <code class="literal">gpt-35-turbo</code>
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45888098640352">
									<p>
										Amazon Bedrock (<code class="literal">bedrock</code>)
									</p>
									</td><td align="left" valign="top" headers="idm45888100748096">
									<p>
										<code class="literal">anthropic.claude-3-5-sonnet-20241022-v2:0</code>, <code class="literal">meta.llama3-1-70b-instruct-v1:0</code>
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45888098640352">
									<p>
										Google Gemini (<code class="literal">google</code>)
									</p>
									</td><td align="left" valign="top" headers="idm45888100748096">
									<p>
										<code class="literal">gemini-2.0-flash-exp</code>, <code class="literal">gemini-1.5-pro</code>
									</p>
									</td></tr><tr><td align="left" valign="top" headers="idm45888098640352">
									<p>
										Ollama (<code class="literal">ollama</code>)
									</p>
									</td><td align="left" valign="top" headers="idm45888100748096">
									<p>
										<code class="literal">llama3.1</code>, <code class="literal">codellama</code>, <code class="literal">mistral</code>
									</p>
									</td></tr></tbody></table></div></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The availability of public LLM models is maintained by the respective LLM provider.
				</p></div></div></section><section class="section" id="persistent-volumes_getting-started"><div class="titlepage"><div><div><h2 class="title">2.2. Persistent volume requirements</h2></div></div></div><p class="_abstract _abstract">
				The Solution Server component requires a backend database to store code changes from previous analyses.
			</p><p>
				If you plan to enable Solution Server, you must create a <code class="literal">5Gi</code> <code class="literal">RWO</code> persistent volume used by the Developer Lightspeed for MTA database. See <a class="link" href="https://docs.redhat.com/en/documentation/migration_toolkit_for_applications/7.3/html/user_interface_guide/mta-7-installing-web-console-on-openshift_user-interface-guide#openshift-persistent-volume-requirements_user-interface-guide">Persistent volume requirements</a> for more information.
			</p></section><section class="section" id="installation_getting-started"><div class="titlepage"><div><div><h2 class="title">2.3. Installation</h2></div></div></div><p class="_abstract _abstract">
				You can install the Migration Toolkit for Applications (MTA) 8.0.0 Visual Studio (VS) Code plug-in from the <a class="link" href="https://marketplace.visualstudio.com/search?term=migration%20toolkit&amp;target=VSCode&amp;category=All%20categories&amp;sortBy=Relevance">VS Code marketplace</a>.
			</p><p>
				You can use the MTA VS Code plug-in to perform analysis and optionally enable Red Hat Developer Lightspeed for migration toolkit for applications to use generative AI capabilities. You can fix code issues before migrating the application to target technologies by using the generative AI capabilities.
			</p></section><section class="section" id="how-to-use-developer-lightspeed_getting-started"><div class="titlepage"><div><div><h2 class="title">2.4. How to use Developer Lightspeed for MTA</h2></div></div></div><p class="_abstract _abstract">
				You can opt to use Red Hat Developer Lightspeed for migration toolkit for applications features to request a code fix suggestion after running a static code analysis of an application. Developer Lightspeed for MTA augments the manual changes made to code throughout your organization in different migration waves and creates a context that is shared with a large language model (LLM). The LLM suggests code resolutions based on the issue description, context, and previous examples of code changes to resolve issues.
			</p><p>
				To make code changes by using the LLM, you must enable the generative AI option, along with either the Solution Server or the Agent AI. The configurations that you complete before you request code fixes depend on how you prefer to request code resolutions.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you make any change after enabling the generative AI settings in the extension, you must restart the extension for the change to take effect.
				</p></div></div><p>
				To use the Solution Server for code fix suggestions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Create a secret for your LLM key in the OCP cluster.
					</li><li class="listitem">
						Enable the Solution Server in the Tackle custom resource (CR).
					</li><li class="listitem">
						Configure the LLM base URL and model in the Tackle CR.
					</li><li class="listitem">
						Enable the generative AI option in the MTA extension settings.
					</li><li class="listitem">
						Add the Solution Server configuration in the <code class="literal">settings.json</code> file.
					</li><li class="listitem">
						Configure the profile settings and activate the LLM provider in the <code class="literal">provider-settings.yaml</code> file.
					</li></ul></div><p>
				To use the agent mode for code fix suggestions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Enable the generative AI and the agent mode in the MTA extension settings.
					</li><li class="listitem">
						Configure the profile settings and activate the LLM provider in the <code class="literal">provider-settings.yaml</code> file.
					</li></ul></div></section><section class="section" id="example-code-generation_getting-started"><div class="titlepage"><div><div><h2 class="title">2.5. Generating code fix suggestions example</h2></div></div></div><p class="_abstract _abstract">
				This example will walk you through generating code fixes for a Java application that must be migrated to the target technology <code class="literal">quarkus</code>. To generate resolutions for issues in the code, we use the Agentic AI mode and the <code class="literal">my-model</code> as the large language model (LLM) that you deployed in OpenShift AI.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open the <code class="literal">my-Java</code> project in Visual Studio (VS) Code.
					</li><li class="listitem">
						Download the Red Hat Developer Lightspeed for migration toolkit for applications extension from the <a class="link" href="https://marketplace.visualstudio.com/search?term=migration%20toolkit&amp;target=VSCode&amp;category=All%20categories&amp;sortBy=Relevance">VS Code marketplace</a>.
					</li><li class="listitem"><p class="simpara">
						Open Command Palette:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Type <code class="literal">Ctrl+Shift+P</code> in Windows and Linux systems.
							</li><li class="listitem">
								Type <code class="literal">Cmd+Shift+P</code> in Mac systems.
							</li></ol></div></li><li class="listitem">
						Type <code class="literal">Preferences: Open Settings (UI)</code> in the Command Palette to open the VS Code settings and select <code class="literal">Extensions &gt; MTA</code>.
					</li><li class="listitem">
						Select <code class="literal">Gen AI:Agent Mode</code>.
					</li><li class="listitem">
						In the Developer Lightspeed for MTA extension, click <code class="literal">Open Analysis View</code>.
					</li><li class="listitem">
						Type <code class="literal">MTA: Manage Analysis Profile</code> in the Command Palette to open the analysis profile page.
					</li><li class="listitem"><p class="simpara">
						Configure the following fields:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								<span class="strong strong"><strong>Profile Name</strong></span>: Type a profile name
							</li><li class="listitem">
								<span class="strong strong"><strong>Target Technologies</strong></span>: <code class="literal">quarkus</code>
							</li><li class="listitem">
								<span class="strong strong"><strong>Custom Rules</strong></span>: Select custom rules if you want to include them while running the analysis. By default, Developer Lightspeed for MTA enables <span class="strong strong"><strong>Use Default Rules</strong></span> for <code class="literal">quarkus</code>.
							</li></ol></div></li><li class="listitem">
						Close the profile manager.
					</li><li class="listitem">
						Type <code class="literal">MTA: Open the Gen AI model provider configuration file</code> in the Command Palette.
					</li><li class="listitem"><p class="simpara">
						Configure the following in the <code class="literal">provider-settings</code> file and close it:
					</p><pre class="programlisting language-yaml">models:
  openshift-example-model: &amp;active
    environment:
      OPENAI_API_KEY: "&lt;Server's OPENAI_API_KEY&gt;"
      CA_BUNDLE: "&lt;Servers CA Bundle path&gt;"
    provider: "ChatOpenAI"
    args:
      model: "my-model"
      configuration:
        baseURL: "https://&lt;serving-name&gt;-&lt;data-science-project-name&gt;.apps.konveyor-ai.example.com/v1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must change the <code class="literal">provider-setting</code> configuration if you plan to use a different LLM provider.
						</p></div></div></li><li class="listitem">
						Type <code class="literal">MTA: Open Analysis View</code> in the Command Palette.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Start</strong></span> to start the MTA RPC server.
					</li><li class="listitem">
						Select the profile you configured.
					</li><li class="listitem"><p class="simpara">
						Click <span class="strong strong"><strong>Run Analysis</strong></span> to scan the Java application.
					</p><p class="simpara">
						MTA identifies the issues in the code.
					</p></li><li class="listitem"><p class="simpara">
						Click the solutions icon (
						<span class="inlinemediaobject"><img src="images/topics/images/solutions-icon.png" alt="solutions icon"/></span>
						) in an issue to request suggestions to resolve the issue.
					</p><p class="simpara">
						Developer Lightspeed for MTA streams the issue description, a preview of the code changes that resolve the issue, and the file(s) in which the changes are to be made.
					</p><p class="simpara">
						You can review the code changes in the editor and accept or reject the changes. If you accept the changes, Developer Lightspeed for MTA creates a new file with the accepted code changes.
					</p></li><li class="listitem"><p class="simpara">
						Click <span class="strong strong"><strong>Continue</strong></span> to allow Developer Lightspeed for MTA to run a follow-up analysis.
					</p><p class="simpara">
						This round of analysis detects lint issues, compilation issues, or diagnostic issues that may have occurred when you accepted the suggested code change.
					</p><p class="simpara">
						Repeat the review and accept or reject the resolutions. Developer Lightspeed for MTA continues to run repeated iterations of scan if you allow until all issues are resolved.
					</p></li></ol></div></section></section><section class="chapter" id="solution-server-configurations_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Solution Server configurations</h1></div></div></div><p class="_abstract _abstract">
			Solution server is a component that allows Developer Lightspeed for MTA to build a collective memory of source code changes from all analysis performed in an organization. When you request code fix for issues in the Visual Studio (VS) Code, the Solution Server augments previous patterns of how source code changed to resolve issues (also called solved examples) that were similar to those in the current file, and suggests a resolution that has a higher confidence level derived from previous solutions. After you accept a suggested code fix, the solution server works with the large language model (LLM) to improve the hints about the issue that becomes part of the context. An improved context enables the LLM to generate more reliable code fix suggestions in future cases.
		</p><p>
			The Solution Server delivers two primary benefits to users:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<span class="strong strong"><strong>Contextual Hints</strong></span>: It surfaces examples of past migration solutions — including successful user modifications and accepted fixes — offering actionable hints for difficult or previously unsolved migration problems.
				</li><li class="listitem">
					<span class="strong strong"><strong>Migration Success Metrics</strong></span>: It exposes detailed success metrics for each migration rule, derived from real-world usage data. These metrics can be used by IDEs or automation tools to present users with a “confidence level” or likelihood of Developer Lightspeed for MTA successfully migrating a given code segment.
				</li></ul></div><p>
			Solution Server is an optional component in Developer Lightspeed for MTA. You must complete the following configurations before you can place a code resolution request.
		</p><div class="table" id="idm45888074350880"><p class="title"><strong>Table 3.1. Configurable large language models and providers in the Tackle custom resource</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45888079653808" scope="col">LLM Provider (Tackle CR value)</th><th align="left" valign="top" id="idm45888079652896" scope="col">Large language model examples for Tackle CR configuration</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45888079653808">
						<p>
							OpenShift AI platform
						</p>
						</td><td align="left" valign="top" headers="idm45888079652896">
						<p>
							Models deployed in an OpenShift AI cluster that can be accessed by using Open AI-compatible API
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45888079653808">
						<p>
							Open AI (<code class="literal">openai</code>)
						</p>
						</td><td align="left" valign="top" headers="idm45888079652896">
						<p>
							<code class="literal">gpt-4</code>, <code class="literal">gpt-4o</code>, <code class="literal">gpt-4o-mini</code>, <code class="literal">gpt-3.5-turbo</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45888079653808">
						<p>
							Azure OpenAI (<code class="literal">azure_openai</code>)
						</p>
						</td><td align="left" valign="top" headers="idm45888079652896">
						<p>
							<code class="literal">gpt-4</code>, <code class="literal">gpt-35-turbo</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45888079653808">
						<p>
							Amazon Bedrock (<code class="literal">bedrock</code>)
						</p>
						</td><td align="left" valign="top" headers="idm45888079652896">
						<p>
							<code class="literal">anthropic.claude-3-5-sonnet-20241022-v2:0</code>, <code class="literal">meta.llama3-1-70b-instruct-v1:0</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45888079653808">
						<p>
							Google Gemini (<code class="literal">google</code>)
						</p>
						</td><td align="left" valign="top" headers="idm45888079652896">
						<p>
							<code class="literal">gemini-2.0-flash-exp</code>, <code class="literal">gemini-1.5-pro</code>
						</p>
						</td></tr><tr><td align="left" valign="top" headers="idm45888079653808">
						<p>
							Ollama (<code class="literal">ollama</code>)
						</p>
						</td><td align="left" valign="top" headers="idm45888079652896">
						<p>
							<code class="literal">llama3.1</code>, <code class="literal">codellama</code>, <code class="literal">mistral</code>
						</p>
						</td></tr></tbody></table></div></div><section class="section" id="tackle-llm-secret_solution-server-configurations"><div class="titlepage"><div><div><h2 class="title">3.1. Configuring the model secret key</h2></div></div></div><p class="_abstract _abstract">
				You must configure the Kubernetes secret for the large language model (LLM) provider in the OpenShift Container Platform project where you installed the MTA operator.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can replace <code class="literal">oc</code> in the following commands with <code class="literal">kubectl</code>.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a credentials secret named <code class="literal">kai-api-keys</code> in the <code class="literal">openshift-mta</code> project.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								For Amazon Bedrock as the provider, type:
							</p><pre class="programlisting language-terminal">oc create secret generic aws-credentials \
 --from-literal=AWS_ACCESS_KEY_ID=&lt;YOUR_AWS_ACCESS_KEY_ID&gt; \
 --from-literal=AWS_SECRET_ACCESS_KEY=&lt;YOUR_AWS_SECRET_ACCESS_KEY&gt;</pre></li><li class="listitem"><p class="simpara">
								For Azure OpenAI as the provider, type:
							</p><pre class="programlisting language-terminal">oc create secret generic kai-api-keys -n openshift-mta \
 --from-literal=AZURE_OPENAI_API_KEY='&lt;YOUR_AZURE_OPENAI_API_KEY&gt;'</pre></li><li class="listitem"><p class="simpara">
								For the Google as the provider, type:
							</p><pre class="programlisting language-terminal">oc create secret generic kai-api-keys -n openshift-mta \
 --from-literal=GEMINI_API_KEY='&lt;YOUR_GOOGLE_API_KEY&gt;'</pre></li><li class="listitem"><p class="simpara">
								For the OpenAI-compatible providers, type:
							</p><pre class="programlisting language-terminal">oc create secret generic kai-api-keys -n openshift-mta \
 --from-literal=OPENAI_API_BASE='https://example.openai.com/v1' \
 --from-literal=OPENAI_API_KEY='&lt;YOUR_OPENAI_KEY&gt;'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You can also set the base URL as the <code class="literal">kai_llm_baseurl</code> variable in the Tackle custom resource.
								</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
						(Optional) Force a reconcile so that the MTA operator picks up the secret immediately
					</p><pre class="programlisting language-terminal">kubectl patch tackle tackle -n openshift-mta --type=merge -p \
'{"metadata":{"annotations":{"konveyor.io/force-reconcile":"'"$(date +%s)"'"}}}'</pre></li></ol></div></section><section class="section" id="tackle-enable-dev-lightspeed_solution-server-configurations"><div class="titlepage"><div><div><h2 class="title">3.2. Enabling Developer Lightspeed for MTA in Tackle custom resource</h2></div></div></div><p class="_abstract _abstract">
				Solution Server integrates with the MTA Hub backend component to use the database and volumes necessary to store and retrieve the solved examples.
			</p><p>
				To enable Solution Server and other AI configurations in the Red Hat Developer Lightspeed for migration toolkit for applications VS Code extension, you must modify the Tackle custom resource (CR) with additional parameters.
			</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						You deployed an additional RWO volume for the <code class="literal">Developer Lightspeed for MTA-database</code> if you want to use Developer Lightspeed for MTA. See <a class="link" href="https://docs.redhat.com/en/documentation/migration_toolkit_for_applications/7.3/html/user_interface_guide/mta-7-installing-web-console-on-openshift_user-interface-guide#openshift-persistent-volume-requirements_user-interface-guide">Persistent volume requirements</a> for more information.
					</li><li class="listitem">
						You installed the MTA operator v8.0.0.
					</li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the OpenShift Container Platform cluster and switch to the <code class="literal">openshift-mta</code> project.
					</li><li class="listitem"><p class="simpara">
						Edit the Tackle CR settings in the <code class="literal">tackle_hub.yml</code> file with the following command:
					</p><pre class="programlisting language-terminal">oc edit tackle</pre></li><li class="listitem"><p class="simpara">
						Enter applicable values for <code class="literal">kai_llm_provider</code> and <code class="literal">kai_llm_model</code> variables.
					</p><pre class="programlisting language-yaml">---
kind: Tackle
apiVersion: tackle.konveyor.io/v1alpha1
metadata:
  name: mta
  namespace: openshift-mta
spec:
  kai_solution_server_enabled: true
  kai_llm_provider: &lt;provider-name&gt;
  # optional, pick a suitable model for your provider
  kai_llm_model: &lt;model-name&gt;
...</pre></li><li class="listitem"><p class="simpara">
						Apply the Tackle CR by in the <code class="literal">openshift-mta</code> project using the following command.
					</p><pre class="programlisting language-terminal"> $ oc apply -f tackle_hub.yaml</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Enter the following command to verify the Developer Lightspeed for MTA resources deployed for Solution Server.
					</p><pre class="programlisting language-terminal">oc get deploy,svc -n openshift-mta | grep -E 'kai-(api|db|importer)'</pre></li></ol></div></section></section><section class="chapter" id="configuring-llm_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Configuring large language models for analysis</h1></div></div></div><p class="_abstract _abstract">
			Developer Lightspeed for MTA provides the large language model (LLM) with the contextual prompt, migration hints, and solved examples to generate suggestions for resolving issues identified in the current code.
		</p><p>
			Developer Lightspeed for MTA is designed to be model agnostic. It works with LLMs that are run in different environments (in local containers, as local AI, or as a shared service) to support analyzing Java applications in a wide range of scenarios. You can choose an LLM from well-known providers, local models that you run from Ollama or Podman desktop, and OpenAI API compatible models.
		</p><p>
			The code fix suggestions produced to resolve issues detected through an analysis depends on the LLM’s capabilities.
		</p><p>
			You can run an LLM from the following generative AI providers:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					OpenAI
				</li><li class="listitem">
					Azure OpenAI
				</li><li class="listitem">
					Google Gemini
				</li><li class="listitem">
					Amazon Bedrock
				</li><li class="listitem">
					Ollama
				</li></ul></div><p>
			You can also run OpenAI API-compatible LLMs deployed as:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					A service in your OpenShift AI cluster
				</li><li class="listitem">
					Locally in the Podman AI Lab in your system.
				</li></ul></div><section class="section" id="llm-service-openshift-ai_configuring-llm"><div class="titlepage"><div><div><h2 class="title">4.1. Deploying an LLM as a service in an OpenShift AI cluster</h2></div></div></div><p class="_abstract _abstract">
				The code suggestions from Red Hat Developer Lightspeed for migration toolkit for applications differ based on the large language model (LLM) that you use. Therefore, you may want to use an LLM that caters to your specific requirements.
			</p><p>
				Developer Lightspeed for MTA integrates with LLMs that are deployed as a scalable service on OpenShift AI clusters. These deployments provide you with granular control over resources such as compute, cluster nodes, and auto-scaling Graphical Processing Units (GPUs) while enabling you to leverage LLMs to resolve code issues at a large scale.
			</p><p>
				An example workflow for configuring an LLM service on OpenShift AI broadly requires the following configurations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Installing and configuring the following infrastructure resources:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								OpenShift Container Platform cluster and installing the OpenShift AI Operator
							</li><li class="listitem">
								Configure a GPU machineset
							</li><li class="listitem">
								(Optional) Configure an auto scaler custom resource (CR) and a machine scaler CR
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Configuring OpenShift AI platform
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure a data science project
							</li><li class="listitem">
								Configure a serving runtime
							</li><li class="listitem">
								Configure an accelerator profile
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Deploying the LLM through OpenShift AI
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Uploading your model to an AWS compatible bucket
							</li><li class="listitem">
								Add a data connection
							</li><li class="listitem">
								Deploy the LLM in your OpenShift AI data science project
							</li><li class="listitem">
								Export the SSL certificate, <code class="literal">OPENAI_API_BASE</code> URL and other environment variables to access the LLM
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Preparing the LLM for analysis
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure an OpenAI API key
							</li><li class="listitem">
								Update the OpenAI API key and the base URL in <code class="literal">provider-settings.yaml</code>.
							</li></ul></div></li></ul></div><p>
				See <a class="link" href="#llm-provider-settings_configuring-llm" title="4.2. Configuring LLM provider settings">Configuring LLM provider settings</a> to configure the base URL and the LLM API key in the Developer Lightspeed for MTA VS Code extension.
			</p></section><section class="section" id="llm-provider-settings_configuring-llm"><div class="titlepage"><div><div><h2 class="title">4.2. Configuring LLM provider settings</h2></div></div></div><p class="_abstract _abstract">
				Red Hat Developer Lightspeed for migration toolkit for applications is large language model (LLM) agnostic and integrates with an LLM of your choice. To enable Developer Lightspeed for MTA to access your large language model (LLM), you must enter the LLM provider configurations in the <code class="literal">provider-settings.yaml</code> file.
			</p><p>
				The <code class="literal">provider-settings.yaml</code> file contains a list of LLM providers that are suppored by default. The mandatory environment variables are different for each LLM provider. Depending on the provider that you choose, you can configure additional environment variables for a model in the <code class="literal">provider-settings.yaml</code> file. You can also enter a new provider with the required environment variables, the base URL, and the model name.
			</p><p>
				The provider settings file is available in the Developer Lightspeed for MTA Visual Studio (VS) Code extension.
			</p><p>
				Access the <code class="literal">provider-settings.yaml</code> from the VS Code Command Palette by typing <code class="literal">Open the GenAI model provider configuration file</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You can select one provider from the list by using the <code class="literal">&amp;active</code> anchor in the name of the provider. To use a model from another provider, move the <code class="literal">&amp;active</code> anchor to <span class="emphasis"><em><span class="strong strong"><strong>one</strong></span></em></span> of the desired provider blocks.
				</p></div></div><p>
				For a model named "my-model" deployed in OpenShift AI with "example-model" as the serving name:
			</p><pre class="programlisting language-yaml">models:
  openshift-example-model: &amp;active
    environment:
      CA_BUNDLE: "&lt;Servers CA Bundle path&gt;"
    provider: "ChatOpenAI"
    args:
      model: "my-model"
      configuration:
        baseURL: "https://&lt;serving-name&gt;-&lt;data-science-project-name&gt;.apps.konveyor-ai.example.com/v1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When you change the <code class="literal">model</code> deployed in OpenShift AI, you must also change the <code class="literal">model</code> argument and the <code class="literal">baseURL</code> endpoint.
				</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you want to select a public LLM provider, you must move the <code class="literal">&amp;active</code> anchor to the desired block and change the provider arguments.
				</p></div></div><p>
				For an OpenAI model:
			</p><pre class="programlisting language-yaml">OpenAI: &amp;active
    environment:
      OPENAI_API_KEY: "&lt;your-API-key&gt;" # Required
    provider: ChatOpenAI
    args:
      model: gpt-4o # Required</pre><p>
				For Azure OpenAI:
			</p><pre class="programlisting language-yaml">AzureChatOpenAI: &amp;active
    environment:
      AZURE_OPENAI_API_KEY: "" # Required
    provider: AzureChatOpenAI
    args:
      azureOpenAIApiDeploymentName: "" # Required
      azureOpenAIApiVersion: "" # Required</pre><p>
				For Amazon Bedrock:
			</p><pre class="programlisting language-yaml">AmazonBedrock: &amp;active
    environment:
      ## May have to use if no global `~/.aws/credentials`
      AWS_ACCESS_KEY_ID: "" # Required if a global ~/.aws/credentials file is not present
      AWS_SECRET_ACCESS_KEY: "" # Required if a global ~/.aws/credentials file is not present
      AWS_DEFAULT_REGION: "" # Required
    provider: ChatBedrock
    args:
      model: meta.llama3-70b-instruct-v1:0 # Required</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					It is recommended to use the <a class="link" href="https://aws.amazon.com/cli/">AWS CLI</a> and verify that you have command line access to AWS services before you proceed with the <code class="literal">provider-settings</code> configurations.
				</p></div></div><p>
				For Google Gemini:
			</p><pre class="programlisting language-yaml">GoogleGenAI: &amp;active
    environment:
      GOOGLE_API_KEY: "" # Required
    provider: ChatGoogleGenerativeAI
    args:
      model: gemini-2.5-pro # Required</pre><p>
				For Ollama:
			</p><pre class="programlisting language-yaml">models:
  ChatOllama: &amp;active
    provider: "ChatOllama"
    args:
      model: "granite-code:8b-instruct"
      baseUrl: "127.0.0.1:11434" # example URL</pre></section><section class="section" id="configuring-llm-podman_configuring-llm"><div class="titlepage"><div><div><h2 class="title">4.3. Configuring the LLM in Podman Desktop</h2></div></div></div><p class="_abstract _abstract">
				The Podman AI lab extension enables you to use an open-source model from a curated list of models and use it locally in your system.
			</p><p>
				The code fix suggestions generated by a model depends on the model’s capabilities. Models deployed through the Podman AI Lab were found to be insufficient for the complexity of code changes required to fix issues discovered by MTA. You must not use such models in a production environment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed <a class="link" href="https://podman-desktop.io/docs/installation">Podman Desktop</a> in your system.
					</li><li class="listitem">
						You completed initial configurations in Developer Lightspeed for MTA required for the analysis.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Go to the Podman AI Lab extension and click <span class="strong strong"><strong>Catalog</strong></span> under <span class="strong strong"><strong>Models</strong></span>.
					</li><li class="listitem">
						Download one or more models.
					</li><li class="listitem">
						Go to <span class="strong strong"><strong>Services</strong></span> and click <span class="strong strong"><strong>New Model Service</strong></span>.
					</li><li class="listitem">
						Select a model that you downloaded in the <span class="strong strong"><strong>Model</strong></span> drop down menu and click <span class="strong strong"><strong>Create Service</strong></span>.
					</li><li class="listitem">
						Click the deployed model service to open the <span class="strong strong"><strong>Service Details</strong></span> page.
					</li><li class="listitem">
						Note the server URL and the model name. You must configure these specifications in the Developer Lightspeed for MTA extension.
					</li><li class="listitem"><p class="simpara">
						Export the inference server URL as follows:
					</p><pre class="programlisting language-terminal">export OPENAI_API_BASE=&lt;server-url&gt;</pre></li><li class="listitem">
						In the Developer Lightspeed for MTA extension, type <code class="literal">Open the GenAI model provider configuration file</code> in the Command Palette to open the <code class="literal">provider-settings.yaml</code> file.
					</li><li class="listitem"><p class="simpara">
						Enter the model details from Podman Desktop. For example, use the following configuration for a Mistral model.
					</p><pre class="programlisting language-yaml">podman_mistral: &amp;active
    provider: "ChatOpenAI"
     environment:
      OPENAI_API_KEY: "unused value"
    args:
      model: "mistral-7b-instruct-v0-2"
      base_url: "http://localhost:35841/v1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The Podman Desktop service endpoint does not need a password but the OpenAI library expects the <code class="literal">OPENAI_API_KEY</code> to be set. In this case, the value of the <code class="literal">OPENAI_API_KEY</code> variable does not matter.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="configuring-dev-lightspeed-ide_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Using MTA with Developer Lightspeed in IDE</h1></div></div></div><p class="_abstract _abstract">
			You must configure the following settings in Red Hat Developer Lightspeed for migration toolkit for applications:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Visual Studio Code IDE settings.
				</li><li class="listitem">
					Profile settings that provide context before you request a code fix for a particular application.
				</li></ul></div><section class="section" id="configuring-developer-lightspeed-ide-settings_configuring-dev-lightspeed-ide"><div class="titlepage"><div><div><h2 class="title">5.1. Configuring the Developer Lightspeed for MTA IDE settings</h2></div></div></div><p class="_abstract _abstract">
				After you install the MTA extension in Visual Studio (VS) Code, you must provide your large language model (LLM) credentials to activate Developer Lightspeed for MTA settings in Visual Studio (VS) Code.
			</p><p>
				Developer Lightspeed for MTA settings are applied to all AI-assisted analysis that you perform by using the MTA extension. The extension settings can be broadly categorized into debugging and logging, Developer Lightspeed for MTA settings, analysis related settings, and solution server settings.
			</p><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
					In addition to the overall prerequisites, you have configured the following:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You completed the solution server configurations in Tackle custom resource if you opt to use the solution server.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Go to the Developer Lightspeed for MTA settings in one of the following ways:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Click <code class="literal">Extensions &gt; MTA Extension for VSCode &gt; Settings</code>
							</li><li class="listitem">
								Type <code class="literal">Ctrl + Shift + P</code> or <code class="literal">Cmd + Shift + P</code> on the search bar to open the Command Palette and enter <code class="literal">Preferences: Open Settings (UI)</code>. Go to <code class="literal">Extensions &gt; MTA</code> to open the settings page.
							</li></ol></div></li><li class="listitem">
						Configure the settings described in the following table:
					</li></ol></div><div class="table" id="idm45888077597152"><p class="title"><strong>Table 5.1. Developer Lightspeed for MTA extension settings</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 40%; " class="col_1"/><col style="width: 60%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45888077593360" scope="col">Settings</th><th align="left" valign="top" id="idm45888077592496" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Log level
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Set the log level for the MTA binary. The default log level is <code class="literal">debug</code>. The log level increases or decreases the verbosity of logs.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Analyzer path
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Specify an MTA custom binary path. If you do not provide a path, Developer Lightspeed for MTA uses the default path to the binary.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Auto Accept on Save
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								This option is enabled by default. When you accept the changes suggested by the LLM, the updated code is saved automatically in a new file. Disable this option if you want to manually save the new file after accepting the suggested code changes.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Gen AI:Enabled
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								This option is enabled by default. It enables you to get code fixes by using Developer Lightspeed for MTA with a large language model.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Gen AI: Agent mode
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Enable the experimental Agentic AI flow for analysis. Developer Lightspeed for MTA runs an automated analysis of a file to identify issues and suggest resolutions. After you accept the solutions, Developer Lightspeed for MTA makes the changes in the code and re-analyzes the file.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Gen AI: Excluded diagnostic sources
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Add diagnostic sources in the <code class="literal">settings.json</code> file. The issues generated by such diagnostic sources are excluded from the automated Agentic AI analysis.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Cache directory
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Specify the path to a directory in your filesystem to store cached responses from the LLM.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Trace directory
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Configure the absolute path to the directory that contains the saved LLM interaction.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Trace enabled
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Enable to trace MTA communication with the LLM model. Traces are stored in the trace directory that you configured.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Demo mode
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Enable to run Developer Lightspeed for MTA in demo mode that uses the LLM responses saved in the <code class="literal">cache</code> directory for analysis.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Solution Server:URL
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Edit the configurations for solution server in <code class="literal">settings.json</code>:
							</p>
							<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										“enabled”: Enter a boolean value. Set <code class="literal">true</code> for connecting the Solution Server client (Developer Lightspeed for MTA extension) to the Solution Server.
									</li><li class="listitem">
										“url”: Configure the URL of the Solution Server end point.
									</li><li class="listitem"><p class="simpara">
										“auth”: The authentication settings allows you to configure a list of options to authenticate to the solution server.
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												"enabled": Set to <code class="literal">true</code> to enable authentication. If you enable authentication, then you must configure the Solution Server realm.
											</li><li class="listitem">
												"insecure": Set to <code class="literal">true</code> to skip SSL certificate verification when clients connect to the Solution Server. Set to <code class="literal">false</code> to allow secure connections to the Solution Server.
											</li><li class="listitem">
												"realm": Enter the name of the Keycloak realm for Solution Server. If you enabled authentication for the Solution Server, you must configure a <a class="link" href="https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/26.0/html/server_administration_guide/red_hat_build_of_keycloak_features_and_concepts">Keycloak realm</a> to allow clients to connect to the Solution Server. An administrator can configure SSL for the realm.
											</li></ul></div></li></ul></div>
							</td></tr><tr><td align="left" valign="top" headers="idm45888077593360">
							<p>
								Debug:Webview
							</p>
							</td><td align="left" valign="top" headers="idm45888077592496">
							<p>
								Enable debug level logging for Webview message handling in VS Code.
							</p>
							</td></tr></tbody></table></div></div><p>
				See <a class="link" href="#configuring-solution-server-settings-file_configuring-dev-lightspeed-ide" title="5.2. Configuring the solution server settings">Configuring the solution server settings</a> for an example Solution Server configuration.
			</p></section><section class="section" id="configuring-solution-server-settings-file_configuring-dev-lightspeed-ide"><div class="titlepage"><div><div><h2 class="title">5.2. Configuring the solution server settings</h2></div></div></div><p class="_abstract _abstract">
				You need a Keycloak realm and the solution server URL to connect Developer Lightspeed for MTA extension with the Solution Server.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The Solution Server URL is available.
					</li><li class="listitem">
						An administrator configured the Keycloak realm for the Solution Server.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Type <code class="literal">Ctrl + Shift + P</code> or <code class="literal">Cmd + Shift + P</code> on the search bar and enter <code class="literal">Preferences:Open User Settings (JSON)</code>.
					</li><li class="listitem">
						In the <code class="literal">settings.json</code> file, enter <code class="literal">Ctrl + SPACE</code> to enable the auto-complete for the Solution Server configurable fields.
					</li><li class="listitem"><p class="simpara">
						Modify the following configuration as necessary:
					</p><pre class="programlisting language-yaml">{
    "mta-vscode-extension.solutionServer": {

        "url": "https://mta-openshift-mta-kai.apps.konveyor-ai.example.com/hub/services/kai/api",

        "enabled": true,
        "auth": {

           "enabled": true, #you must enter the username and password
           "insecure": true,
           "realm": "mta"
        },

    }
}</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							When you enable Solution Server authentication for the first time, you must enter the <code class="literal">username</code> and <code class="literal">password</code> in the VS Code search bar.
						</p></div></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						Enter <code class="literal">MTA: Restart Solution Server</code> in the Command Palette to restart the Solution Server.
					</p></div></div></li></ol></div></section><section class="section" id="configuring-developer-lightspeed-profile-settings_configuring-dev-lightspeed-ide"><div class="titlepage"><div><div><h2 class="title">5.3. Configuring the Developer Lightspeed for MTA profile settings</h2></div></div></div><p class="_abstract _abstract">
				You can use the Visual Studio (VS) Code plugin to run an analysis to discover issues in the code. You can optionally enable Red Hat Developer Lightspeed for migration toolkit for applications to get AI-assisted code suggestions.
			</p><p>
				To generate code changes using Developer Lightspeed for MTA, you must configure a profile that contains all the necessary configurations, such as source and target technologies and the API key to connect to your chosen large language model (LLM).
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You completed the solution server configurations in Tackle custom resource if you opt to use the solution server.
					</li><li class="listitem">
						You opened a Java project in your VS Code workspace.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Open the <code class="literal">MTA View Analysis</code> page in either of the following ways:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Click the book icon on the <code class="literal">MTA: Issues</code> pane of the MTA extension.
							</li><li class="listitem">
								Type <code class="literal">Ctrl + Shift + P</code> or <code class="literal">Cmd + Shift + P</code> on the search bar to open the Command Palette and enter <code class="literal">MTA:Open Analysis View</code>.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Click the settings button on the <code class="literal">MTA View Analysis</code> page to configure a profile for your project. The <code class="literal">Get Ready to Analyze</code> pane lists the follwoing basic configurations required for an analysis:
					</p><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
							After you complete the profile configuration, close the <code class="literal">Get Ready to Analyze</code> pane. You can verify that your configuration works by running an analysis.
						</p></div></li></ol></div><div class="table" id="idm45888074755344"><p class="title"><strong>Table 5.2. Developer Lightspeed for MTA profile settings</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"/><col style="width: 80%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm45888074751440" scope="col">Profile settings</th><th align="left" valign="top" id="idm45888074750576" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm45888074751440">
							<p>
								Select profile
							</p>
							</td><td align="left" valign="top" headers="idm45888074750576">
							<p>
								Create a profile that you can reuse for multiple analyses. The profile name is part of the context provided to the LLM for analysis.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888074751440">
							<p>
								Configure label selector
							</p>
							</td><td align="left" valign="top" headers="idm45888074750576">
							<p>
								A label selector filters rules for analysis based on the source or target technology.
							</p>
							<p>
								Specify one or more target or source technologies (for example, cloud-readiness). Developer Lightspeed for MTA uses this configuration to determine the rules that are applied to a project during analysis.
							</p>
							<p>
								If you mentioned a new target or a source technology in your custom rule, you can type that name to create and add the new item to the list.
							</p>
							<div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You must configure either target or source tehcnologies before running an analysis.
								</p></div></div>
							</td></tr><tr><td align="left" valign="top" headers="idm45888074751440">
							<p>
								Set rules
							</p>
							</td><td align="left" valign="top" headers="idm45888074750576">
							<p>
								Enable default rules and select your custom rule that you want MTA to use for an analysis. You can use the custom rules in addition to the default rules.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm45888074751440">
							<p>
								Configure generative AI
							</p>
							</td><td align="left" valign="top" headers="idm45888074750576">
							<p>
								This option opens the <code class="literal">provider-settings.yaml</code> file that contains API keys and other parameters for all supported LLMs. By default, Developer Lightspeed for MTA is configured to use OpenAI LLM. To change the model, update the anchor <code class="literal">&amp;active</code> to the desired block. Modify this file with the required arguments, such as the model and API key, to complete the setup.
							</p>
							</td></tr></tbody></table></div></div><p>
				See for <a class="link" href="#llm-provider-settings_configuring-llm" title="4.2. Configuring LLM provider settings">Configuring LLM provider settings</a> to complete the LLM provider configuration.
			</p></section></section><section class="chapter" id="run-dev-lightspeed-analysis_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Running an analysis and resolving issues</h1></div></div></div><p class="_abstract _abstract">
			After you complete the configurations, the next step is running an analysis to identify the issues in the code and generate suggestions to resolve the issues. You can get suggestions to fix code by using Red Hat Developer Lightspeed for migration toolkit for applications.
		</p><p>
			When you run an analysis, MTA displays the issues in the <span class="strong strong"><strong>Analysis Results</strong></span> view.
		</p><p>
			When you request code fix suggestions, Developer Lightspeed for MTA performs the following tasks:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Streams LLM messages that describe the issue description, resolution, and the file in which the updates are applied.
				</li><li class="listitem">
					Generates new files in the <span class="strong strong"><strong>Resolutions</strong></span> pane. These files have the updates to the code to resolve the issues detected in the current analysis. You can review the changes, apply, or revert the updates.
				</li></ul></div><p>
			If you apply all the resolutions, Developer Lightspeed for MTA applies the changes and triggers another analysis to check if there are more issues. Subsequent analysis reports fewer issues and incidents.
		</p><section class="section" id="running-rag-analysis_run-dev-lightspeed-analysis"><div class="titlepage"><div><div><h2 class="title">6.1. Running an Analysis</h2></div></div></div><p class="_abstract _abstract">
				You can run a static code analysis of an application with or without enabling the generative AI features. The RPC (Remote Procedure Call) server runs the analysis to detect all issues in the code for one or more target technologies to which you want to migrate the application.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You opened a Java project in your VS Code workspace.
					</li><li class="listitem">
						You configured an analysis profile on the <span class="strong strong"><strong>MTA Analysis View</strong></span> page.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Click the Developer Lightspeed for MTA extension and click <span class="strong strong"><strong>Open MTA Analysis View</strong></span>.
					</li><li class="listitem">
						Select a profile for the analysis.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Start</strong></span> to start the MTA RPC server.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Run Analysis</strong></span> on the <span class="strong strong"><strong>MTA Analysis View</strong></span> page.
					</li></ol></div></section><section class="section" id="apply-rag-resolution_run-dev-lightspeed-analysis"><div class="titlepage"><div><div><h2 class="title">6.2. Applying resolutions generated by the solution server</h2></div></div></div><p class="_abstract _abstract">
				When you request code resolutions by enabling the Solution Server, an issue displays the success metric when the metric becomes available. A success metric indicates the confidence level in applying the fix suggestion from the LLM based on how many times the update was applied in past analysis.
			</p><p>
				You can review the code updates and edit the suggested code resolutions before accepting the suggestions.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You opened a Java project in your VS Code workspace.
					</li><li class="listitem">
						You configured a profile on the <span class="strong strong"><strong>MTA Analysis View</strong></span> page
					</li><li class="listitem">
						You ran an analysis after enabling solution server.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Review the issues from the <span class="strong strong"><strong>Analysis results</strong></span> space of the <span class="strong strong"><strong>MTA view analysis</strong></span> page by the following tabs:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								<span class="strong strong"><strong>All</strong></span>: lists all incidents identified in your project.
							</li><li class="listitem">
								<span class="strong strong"><strong>Files</strong></span>: lists all the files in your project for which the analysis identified issues that must be resolved.
							</li><li class="listitem">
								<span class="strong strong"><strong>Issues</strong></span>: lists all issues across different files in your project.
							</li></ol></div></li><li class="listitem">
						Use the <span class="strong strong"><strong>Category</strong></span> drop down to filter issues based on how crucial the fix is for the target migration. You can filter mandatory, potential, and optional issues.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Has Success Rate</strong></span> to check how many times the same issue resolution was accepted in previous analysis.
					</li><li class="listitem">
						Click the solution tool to trigger automated updates to your code. If you applied any category filter, code updates are made for all incidents, specific files, or specific issues based on the filter. Developer Lightspeed for MTA generates new files with the updated code.
					</li><li class="listitem">
						Review and (optionally) edit the code.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Apply all</strong></span> in the <span class="strong strong"><strong>Resolutions</strong></span> pane to permanently apply the changes to your code.
					</li></ol></div></section><section class="section" id="running-agent-analysis_run-dev-lightspeed-analysis"><div class="titlepage"><div><div><h2 class="title">6.3. Generating code resolutions in the agent mode</h2></div></div></div><p class="_abstract _abstract">
				In the agent mode, the Developer Lightspeed for MTA planning agent creates the context for an issue and picks a sub-agent that is most suited to resolve the issue. The sub-agent runs an automated scan to describe how the issue can be resolved and generates files with the updated resolutions in one stream.
			</p><p>
				You can review the updated files and approve or reject the changes to the code. The agent runs another automated analysis to detect new issues in the code that may have occurred because of the accepted changes or diagnostic issues that your tool may generate following a previous analysis. If you allow the process to continue, Developer Lightspeed for MTA runs the stream again and generates a new file with the latest updates.
			</p><p>
				When using the agent mode, you can reject the changes or discontinue the stream but you cannot edit the updated files during the stream.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You opened a Java project in your VS Code workspace.
					</li><li class="listitem">
						You configured an analysis profile on the <span class="strong strong"><strong>MTA Analysis View</strong></span> page.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Verify that agent mode is enabled in one of the following ways:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Type <code class="literal">Ctrl + Shift + P</code> in VS Code search (Linux/Windows system) and <code class="literal">Cmd + Shift + P</code> for Mac to go to the command palette.
							</li><li class="listitem">
								Enter <code class="literal">Preferences: Open User Settings (JSON)</code> to open the <code class="literal">settings.json</code> file.
							</li><li class="listitem"><p class="simpara">
								Ensure that <code class="literal">mta-vscode-extension.genai.agentMode</code> is set to <code class="literal">true</code>.
							</p><p class="simpara">
								OR
							</p></li><li class="listitem">
								Go to <span class="strong strong"><strong>Extensions &gt; Developer Lightspeed for MTA &gt; settings</strong></span>
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Agent Mode</strong></span> option to enable the server.
							</li></ol></div></li><li class="listitem">
						Click the Developer Lightspeed for MTA extension and click <span class="strong strong"><strong>Open MTA Analysis View</strong></span>.
					</li><li class="listitem">
						Select a profile for the analysis.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Start</strong></span> to start the MTA RPC server.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Run Analysis</strong></span> on the <span class="strong strong"><strong>MTA Analysis View</strong></span> page. The <span class="strong strong"><strong>Resolution Details</strong></span> tab opens, where you can view the automated analysis that makes changes in applicable files.
					</li><li class="listitem">
						Click the <span class="strong strong"><strong>Review Changes</strong></span> option to open the editor that shows the diff view of the modified file.
					</li><li class="listitem">
						Review the changes and click <span class="strong strong"><strong>Apply</strong></span> to update the file with all the changes or <span class="strong strong"><strong>Reject</strong></span> to reject all changes. If you applied the changes, then Developer Lightspeed for MTA creates the updated file with code changes.
					</li><li class="listitem">
						Open <span class="strong strong"><strong>Source Control</strong></span> to access the updated file.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Resolution Details</strong></span> view, accept the proposal from Developer Lightspeed for MTA to make further changes. The stream of analysis repeats, after which you can review and accept change. Developer Lightspeed for MTA creates the file with the code changes, and the stream continues until you reject the proposal for further analysis.
					</li></ol></div></section></section><section class="chapter" id="developer-lightspeed-logs_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Debugging Developer Lightspeed for MTA</h1></div></div></div><p class="_abstract _abstract">
			Red Hat Developer Lightspeed for migration toolkit for applications generates logs to debug issues specific to the extension host and the MTA analysis and RPC server. You can also configure the log level for the Developer Lightspeed for MTA in the extension settings. The default log level is <span class="strong strong"><strong>debug</strong></span>.
		</p><p>
			Extension logs are stored as <code class="literal">extension.log</code> with automatic rotation. The maximum size of the log file is 10 MB and three files are retained. Analyzer RPC logs are stored as <code class="literal">analyzer.log</code> without rotation.
		</p><section class="section" id="dev-lightspeed-archive-logs_mta-developer-lightspeed"><div class="titlepage"><div><div><h2 class="title">7.1. Archiving the logs</h2></div></div></div><p>
				To archive the logs as a zip file, type <code class="literal">MTA: Generate Debug Archive</code> in the VS Code Command Palette and select the information type that must be archived as a log file.
			</p><p>
				The archive command allows capturing all relevant log files in a zip archive at the specified location in your project. By default, you can access the archived logs in the .vscode directory of your project.
			</p><p>
				The archival feature helps you to save the following information:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Large language model (LLM) provider configuration: Fields from the provider settings that can be included in the archive. All fields are redacted for security reasons by default. Ensure that you do not expose any secrets.
					</li><li class="listitem">
						LLM model arguments
					</li><li class="listitem">
						LLM traces: If you enabled tracing LLM interactions, you can choose to include LLM traces in the logs.
					</li></ul></div></section><section class="section" id="dev-lightspeed-access-logs_mta-developer-lightspeed"><div class="titlepage"><div><div><h2 class="title">7.2. Accessing the logs</h2></div></div></div><p>
				You can access the logs in the following ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Log file</strong></span>: Type <code class="literal">Developer: Open Extension Logs Folder</code> and open the <code class="literal">redhat.mta-vscode-extension</code> directory that contains the extension log and the analyzer log.
					</li><li class="listitem">
						<span class="strong strong"><strong>Output panel</strong></span>: Select <code class="literal">Developer Lightspeed for MTA</code> from the drop-down menu.
					</li><li class="listitem">
						<span class="strong strong"><strong>Webview logs</strong></span>: You can also inspect webview content by using the webview logs. To access the webview logs, type <code class="literal">Open Webview Developer Tools</code> in the VS Code Command Palette.
					</li></ul></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm45888077825088"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2025 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>
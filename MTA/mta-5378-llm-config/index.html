<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>MTA Developer Lightspeed</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.2"/><meta name="description" content="Use Migration Toolkit for Applications (MTA) Developer Lightspeed for application modernization in your organization by running Artificial Intelligence-driven code updates after a static code analysis for Java applications."/><link rel="next" href="#making-open-source-more-inclusive" title="Making open source more inclusive"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm45677583448224"><div class="titlepage"><div><div class="producttitle"><span class="productname">Migration Toolkit for Applications</span> <span class="productnumber">7.3</span></div><div><h1 class="title">MTA Developer Lightspeed</h1></div><div><h2 class="subtitle">Using the Migration Toolkit for Applications command-line interface to migrate your applications</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat Customer Content Services</span></div></div><div><a href="#idm45677573107456">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Use Migration Toolkit for Applications (MTA) Developer Lightspeed for application modernization in your organization by running Artificial Intelligence-driven code updates after a static code analysis for Java applications.
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="preface"><a href="#making-open-source-more-inclusive">Making open source more inclusive</a></span></li><li><span class="chapter"><a href="#configuring-llm_mta-developer-lightspeed">1. Configuring large language models for analysis</a></span><ul><li><span class="section"><a href="#llm-service-openshift-ai_configuring-llm">1.1. Deploying an LLM as a service in OpenShift Container Platform AI</a></span></li><li><span class="section"><a href="#llm-provider-settings_configuring-llm">1.2. Provider settings configuration</a></span></li><li><span class="section"><a href="#configuring-llm-podman_configuring-llm">1.3. Configuring the LLM in Podman Desktop</a></span></li></ul></li></ul></div><section class="preface" id="making-open-source-more-inclusive"><div class="titlepage"><div><div><h1 class="title">Making open source more inclusive</h1></div></div></div><p class="_abstract _abstract">
			Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see <a class="link" href="https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language">our CTO Chris Wright’s message</a>.
		</p></section><section class="chapter" id="configuring-llm_mta-developer-lightspeed"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Configuring large language models for analysis</h1></div></div></div><p>
			To generate suggestions to resolves issues in the code, Developer Lightspeed for MTA provides the large language model (LLM) with the contextual prompt, migration hints, and solved examples to generate suggestions to resolve issues identified in the current code by running an analysis.
		</p><p>
			Developer Lightspeed for MTA is designed to be model agnostic. It works with LLMs that are run in different environments (in local containers, as local AI, as a shared service) to support analyzing Java applications in a wide range of scenarios. You can choose an LLM from well-known providers, local models that you run from Ollama or Podman desktop, and OpenAI API compatible models.
		</p><p>
			The code fix suggestions produced to resolve issues detected through an analysis depends on the LLM’s capabilities.
		</p><p>
			You can run an LLM from the following generative AI providers:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					OpenAI
				</li><li class="listitem">
					Azure OpenAI
				</li><li class="listitem">
					Google Gemini
				</li><li class="listitem">
					Amazon Bedrock
				</li><li class="listitem">
					Ollama
				</li><li class="listitem">
					Groq
				</li><li class="listitem">
					Anthropic
				</li></ul></div><p>
			You can also run OpenAI API-compatible LLMs deployed as a service in your OpenShift AI cluster or deployed locally in the Podman AI Lab in your system.
		</p><section class="section" id="llm-service-openshift-ai_configuring-llm"><div class="titlepage"><div><div><h2 class="title">1.1. Deploying an LLM as a service in OpenShift Container Platform AI</h2></div></div></div><p class="_abstract _abstract">
				The code suggestions from Red Hat Developer Lightspeed for migration toolkit for applications differ based on the large language model (LLM) that you use. Therefore, you may want to use an LLM that caters to your specific requirements.
			</p><p>
				Developer Lightspeed for MTA integrates with LLMs that are deployed as a scalable service on Red Hat OpenShift Container Platform clusters. These deployments provide you with a granular control over resources such as compute, cluster nodes, and auto-scaling Graphical Processing Units (GPUs) while enabling you to leverage LLMs to perform analysis at a large scale.
			</p><p>
				An example workflow for configuring an LLM service on OpenShift Container Platform AI broadly requires the following configurations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Installing and configuring the following infrastructure resources:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								OpenShift Container Platform cluster and installing the OpenShift Container Platform AI Operator
							</li><li class="listitem">
								Configure a GPU machineset
							</li><li class="listitem">
								(Optional) Configure an auto scaler custom resource (CR) and a machine scaler CR
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Configuring OpenShift Container Platform AI platform
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure a data science project
							</li><li class="listitem">
								Configure a serving runtime
							</li><li class="listitem">
								Configure an accelerator profile
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Deploying the LLM through OpenShift Container Platform AI
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Uploading your model to an AWS compatible bucket
							</li><li class="listitem">
								Add a data connection
							</li><li class="listitem">
								Deploy the LLM in your OCP AI data science project
							</li><li class="listitem">
								Export the SSL certificate, <code class="literal">OPENAI_API_BASE</code> URL and other environment variables to access the LLM
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Preparing the LLM for analysis
					</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
								Configure an OpenAI API key
							</li><li class="listitem">
								Update the OpenAI API key and the base URL in <code class="literal">provider-settings.yaml</code>.
							</li></ul></div></li></ul></div><p>
				See Provider settings configuration to configure the base URL and LLM key in the Developer Lightspeed for MTA VS Code extension.
			</p></section><section class="section" id="llm-provider-settings_configuring-llm"><div class="titlepage"><div><div><h2 class="title">1.2. Provider settings configuration</h2></div></div></div><p class="_abstract _abstract">
				Red Hat Developer Lightspeed for migration toolkit for applications is large language model (LLM) agnostic and intergrates with an LLM of your choice.
			</p><p>
				To enable Red Hat Developer Lightspeed for migration toolkit for applications to access your large language model (LLM), you must enter the LLM provider configurations in the <code class="literal">provider-settings.yaml</code> file.
			</p><p>
				The <code class="literal">provider-settings.yaml</code> file contains a list of LLM providers that are suppored by default. The mandatory environment variables are different for each LLM provider. Depending on the provider that you choose, you can configure additional environment variables for a model in the <code class="literal">provider-settings.yaml</code> file. You can also enter a new provider with the required environment variables, the base URL, and the model name.
			</p><p>
				The provider settings file is available in the Developer Lightspeed for MTA Visual Studio (VS) Code extension.
			</p><p>
				Access the <code class="literal">provider-settings.yaml</code> from the VS Code Command Pallete by typing <code class="literal">Open the GenAI model provider configuration file</code>.
			</p><p>
				You can select one provider from the list by using the <code class="literal">&amp;active</code> anchor in the name of the provider. To use a model from another provider, move the <code class="literal">&amp;active</code> anchor to the desired provider block and restart the solution server on the <code class="literal">Open MTA Analysis View</code> screen.
			</p><p>
				For an OpenAI model:
			</p><pre class="programlisting language-yaml">OpenAI: &amp;active
    environment:
      OPENAI_API_KEY: "&lt;your-API-key&gt;" # Required
    provider: ChatOpenAI
    args:
      model: gpt-4o # Required</pre><p>
				For Azure OpenAI:
			</p><pre class="programlisting language-yaml">AzureChatOpenAI:
    environment:
      AZURE_OPENAI_API_KEY: "" # Required
    provider: AzureChatOpenAI
    args:
      azureOpenAIApiDeploymentName: "" # Required
      azureOpenAIApiVersion: "" # Required</pre><p>
				For Amazon Bedrock:
			</p><pre class="programlisting language-yaml">AmazonBedrock:
    environment:
      ## May have to use if no global `~/.aws/credentials`
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: "" # Required if a global ~/.aws/credentials file is not present
      AWS_SECRET_ACCESS_KEY: "" # Required if a global ~/.aws/credentials file is not present
      AWS_DEFAULT_REGION: "" # Required
    provider: ChatBedrock
    args:
      model: meta.llama3-70b-instruct-v1:0 # Required</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					It is recommended to use the <a class="link" href="https://aws.amazon.com/cli/">AWS CLI</a> and verify that you have command line access to AWS services before you proceed with the <code class="literal">provider-settings</code> configurations.
				</p></div></div><p>
				For Google Gemini:
			</p><pre class="programlisting language-yaml">GoogleGenAI:
    environment:
      GOOGLE_API_KEY: "" # Required
    provider: ChatGoogleGenerativeAI
    args:
      model: gemini-2.5-pro # Required</pre><p>
				For Ollama:
			</p><pre class="programlisting language-yaml">models:
  ChatOllama:
    provider: "ChatOllama"
    args:
      model: "granite-code:8b-instruct"
      baseUrl: "127.0.0.1:11434" # example URL</pre><p>
				For a model named "my-model" deployed in OpenShift Container Platform AI with "example-model" as the serving name:
			</p><pre class="programlisting language-yaml">models:
  openshift-example-model:
    environment:
      CA_BUNDLE: "&lt;Servers CA Bundle path&gt;"
      ALLOW_INSECURE: "true"
    provider: "ChatOpenAI"
    args:
      model: "my-model"
      configuration:
        base_url: "https://&lt;serving-name&gt;-&lt;data-science-project-name&gt;.apps.konveyor-ai.migration.redhat.com/v1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					When you change the <code class="literal">model</code> deployed in OpenShift Container Platform AI, you must also change the <code class="literal">base_url</code> endpoint.
				</p></div></div></section><section class="section" id="configuring-llm-podman_configuring-llm"><div class="titlepage"><div><div><h2 class="title">1.3. Configuring the LLM in Podman Desktop</h2></div></div></div><p class="_abstract _abstract">
				The Podman AI lab extension enables you to use an open-source model from a curated list of models and use it locally in your system.
			</p><p>
				The code fix suggestions generated by a model depends on the model’s capabilities. Models deployed through the Podman AI Lab must were found to be insufficient for the complexity of code changes required to fix issues discovered by MTA. You must not use such models in production environment.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You installed <a class="link" href="https://podman-desktop.io/docs/installation">Podman Desktop</a> in your system.
					</li><li class="listitem">
						You completed initial configurations in Developer Lightspeed for MTA required for the analysis.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Go to the Podman AI Lab extension and click <span class="strong strong"><strong>Catalog</strong></span> under <span class="strong strong"><strong>Models</strong></span>.
					</li><li class="listitem">
						Download one or more models.
					</li><li class="listitem">
						Go to <span class="strong strong"><strong>Services</strong></span> and click <span class="strong strong"><strong>New Model Service</strong></span>.
					</li><li class="listitem">
						Select a model that you downloaded in the <span class="strong strong"><strong>Model</strong></span> drop down menu and click <span class="strong strong"><strong>Create Service</strong></span>.
					</li><li class="listitem">
						Click the deployed model service to open the <span class="strong strong"><strong>Service Details</strong></span> page.
					</li><li class="listitem">
						Note the server URL and the model name. You must configure these specifications in the Developer Lightspeed for MTA extension.
					</li><li class="listitem"><p class="simpara">
						Export the inference server URL as follows:
					</p><pre class="programlisting language-terminal">export OPENAI_API_BASE=&lt;server-url&gt;</pre></li><li class="listitem">
						In the Developer Lightspeed for MTA extension, type <code class="literal">Open the GenAI model provider configuration file</code> in the Command Pallete to open the <code class="literal">provider-settings.yaml</code> file.
					</li><li class="listitem"><p class="simpara">
						Enter the model details from Podman Desktop. For example, use the following configuration for a Mistral model.
					</p><pre class="programlisting language-yaml">podman_mistral:
    provider: "ChatOpenAI"
     environment:
      OPENAI_API_KEY: "unused value"
    args:
      model: "mistral-7b-instruct-v0-2"
      base_url: "http://localhost:35841/v1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The Podman Desktop service endpoint does not need a password but the OpenAI library expects the <code class="literal">OPENAI_API_KEY</code> to be set. In this case, the value of the <code class="literal">OPENAI_API_KEY</code> variable does not matter.
						</p></div></div></li></ol></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm45677573107456"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2025 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>